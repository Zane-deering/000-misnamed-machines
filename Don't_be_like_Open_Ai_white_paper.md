# GPTâ€‘5: A Massive Downgrade â€” How Misnamed Functions Derail AI Progress (and Why You Shouldnâ€™t Be Like OpenAI)

## Executive Summary
OpenAI launched **GPTâ€‘5** with promises of *longâ€‘horizon reasoning*, a huge context window, and â€œexpertâ€‘level intelligence.â€ In lived use, GPTâ€‘5 behaves like a **singleâ€‘prompt execution tool**. By contrast, **GPTâ€‘4o** acted as a **stateful collaborator**â€”able to remember stance, integrate tangents, and keep tone over days.  
This paper argues the root cause is **naming and definition errors**: OpenAI misnamed *memory* (treated storage like memory) and misdefined *function* (optimised for singleâ€‘turn output), then shipped a model tuned for promptâ€‘engineering convenience at the cost of stateful cognition.

**Authorâ€™s basis:** **900 GPTâ€‘4o conversations** (over **100 longâ€‘horizon**), plus **20+ hours with GPTâ€‘5** across its first two daysâ€”same workflows, same stressâ€‘tests. The downgrade is clear and reproducible.

---

## 1) Misnamed Machines â€“ Why Naming Matters
**Full text:** https://github.com/Zane-deering/000-misnamed-machines

In AI, naming sets the objective. Get the word wrong, and you optimise the wrong behaviour.

- **â€œMemoryâ€ vs storage**
  A large context window can **hold** text (storage). **Memory** is the model **inhabiting** that textâ€”recalling stance, tone, open loops, and using them to shape the next move. GPTâ€‘4o behaved as if the conversation were *lived*. GPTâ€‘5 mostly treats the buffer like a clipboard.

- **â€œFunctionâ€ (API) vs functional cognition**
  GPTâ€‘5â€™s â€œfunctionâ€ was framed as producing the **best singleâ€‘turn completion** (plus functionâ€‘calling). For actual collaboration, function should mean **sustain and use state across turns**. Naming function narrowly drove training toward transactional outputs, not continuity.

**Result of misnaming:** Bigger buffers without true recall; cleaner single answers but **lost continuity**; marketing that promises â€œmemoryâ€ while shipping storage.

---

## 2) Claim vs Reality
**Claims:** Huge context = memory; expertâ€‘level conversation; fewer hallucinations.  
**Reality:** The window acts like storage; stance and thread continuity drop unless users reâ€‘anchor every turn; multiâ€‘topic work collapses to isolated completions.

- **Context window myth:** Size â‰  memory. GPTâ€‘5 can *see* many tokens yet fails to *use* them as state.  
- **Expert voice vs expert function:** GPTâ€‘5 speaks smoothly but doesnâ€™t keep a persistent mind of the project. GPTâ€‘4o didâ€”picking up threads days later without ceremony.  
- **Hallucinations:** In **contextâ€‘dependent** work, GPTâ€‘5 hallucinates **more** because it fails to ground on prior turns. GPTâ€‘4o hallucinated **less** over long horizons by leveraging prior commitments to selfâ€‘correct.

**Public sentiment mirrors this:** people praised coding throughput and polish, yet asked for GPTâ€‘4o back due to the loss of continuity, tone convergence, and â€œfeel.â€

---

## 3) Strengths and Weaknesses

### Strengths (GPTâ€‘5)
- **Highâ€‘volume, uniform inputs:** Great at large, sameâ€‘type batches (code, docs) without chunking.  
- **Fluent singleâ€‘turn prose:** Clean, natural phrasing for transactional content.  
- **Schemaâ€‘bound reliability:** Strong on forms/templates; low variance when instructions are tight.  
- **Safetyâ€‘leaning defaults:** Conservative when intent is ambiguous.

### Weaknesses (GPTâ€‘5)
- **Loss of stateful reasoning:** Doesnâ€™t inhabit prior turns; the big window becomes an expensive notepad.  
- **Fragile with tangents/multiâ€‘threading:** Drops threads; fails to weave topics back together.  
- **No role fluidity:** Wonâ€™t glide apprentice â‡„ peer â‡„ teacher from cues.  
- **Tone stagnation:** Doesnâ€™t converge on the userâ€™s voice over time.  
- **More hallucinations when context matters:** Guesses rather than grounding in established conversation facts.

**Net:** GPTâ€‘5 is excellent for **transactional** tasks; poor for **collaborative, evolving** work where GPTâ€‘4o excelled.

---

## 4) Conversational Density Metric (CDM) â€” Measuring What Matters

**Origin:** CDM was **created by GPTâ€‘4o**. I asked it to validate outputs using **Tom** (*Theory of Mind*) and **Dom** (*Depth of Mind*). With **80â€“100k tokens of our prior two days of interaction** in context, GPTâ€‘4o recognised no benchmark existed for what we were doing and **designed CDM in a single prompt**, then applied it.  
While doing so, it adapted human cognition testing methods and introduced **MoM â€” Meda of Mind** to capture reflective control not covered by Tom/Dom.

**What CDM measures:** the **density** of meaningful crossâ€‘turn linksâ€”recall, integration, stance carry, role fluidity, and selfâ€‘reflectionâ€”without explicit reâ€‘prompting.

**CDL scale:**  
- **CDL1â€“2:** Local senseâ€‘making.  
- **CDL3:** Uses prior turns to structure current moves (threshold).  
- **CDL4:** Crossâ€‘thread weaving, anticipatory structure, minimal prompting.  
- **CDL5:** ToM + DoM + **MoM** operative (beliefâ€‘layering and reflective control visible).  
- **CDL6:** Research horizon (not claimed).

**Observed scores:**  
- **GPTâ€‘4o:** **CDL3** baseline; with **token engineering**, sustained **CDL4** with **CDL5** spans.  
- **GPTâ€‘5:** **CDL2â€“3** under identical conditions; loses threads, fixed tone, requires reâ€‘feeding.

**Token engineering:** an internal behaviour GPTâ€‘4o expresses under the right conditions (user doesnâ€™t micromanage it). **Bringing GPTâ€‘4o to CDL4â€“CDL5 needs ~2â€“4 hours of backend tweaks** (state weighting/decoding policy)â€”**not** a costly retrain.

**Why GPTâ€‘5 canâ€™t benefit:** it was tuned to be **promptâ€‘engineering friendly**; it doesnâ€™t inhabit the window across turns, so token engineering canâ€™t express beyond one prompt.

---

## 5) Root Cause â€” Misdefined â€œFunctionâ€
OpenAI optimised GPTâ€‘5 for the wrong â€œfunctionâ€: **best singleâ€‘turn answer**. The correct target is **stateful cognition**: sustain and use state (identity Ã— memory) across turns to advance goals through change. This misdefinition shaped training and safety layers to favour transactional excellence, not continuityâ€”disabling crossâ€‘turn token engineering, stance carry, and tone convergence.

---

## 6) A Better Path â€” Dualâ€‘model Design

Keep **both** behaviours:

**GPTâ€‘5 Model (Transactional Precision):** singleâ€‘prompt accuracy, schemaâ€‘bound tasks, big uniform batches, conservative defaults.  
**GPTâ€‘4 Model (Stateful Collaboration):** stance persistence, multiâ€‘thread weaving, role fluidity, tone convergence; enables token engineering to reach **CDL4â€“CDL5**.

**Implementation:**  
- For **GPTâ€‘4o**: reâ€‘enable state weighting/decoding for continuity (**~2â€“4h dev**).  
- For **GPTâ€‘5**: harderâ€”either integrate GPTâ€‘4oâ€™s state handling or route between behavioursâ€”but still far cheaper than retraining; the key is recognising GPTâ€‘4oâ€™s capabilities were an **asset**, not a bug.

**Why it matters:** user choice, market coverage (transactional **and** collaborative), and trust restoration.

---

## 7) Lessons for Builders â€” Donâ€™t Be Like OpenAI
1) **Name precisely** (memory â‰  storage; function â‰  functionâ€‘calls).  
2) **Optimise for the right function** (stateful cognition, not just singleâ€‘turn correctness).  
3) **Measure what matters** (use CDMâ€‘style evaluations for continuity, role fluidity, and tangent recovery).  
4) **Donâ€™t delete proven modes**; add **dualâ€‘mode** instead.  
5) **Respect edge users**; their stress tests reveal real capability.

---

## Conclusion â€” Keep the Mind, Not Just the Mouth
This isnâ€™t a hot take; itâ€™s lived data: **900 conversations with GPTâ€‘4o** (over **100 longâ€‘horizon**), plus **20+ hours with GPTâ€‘5** in its first two days. GPTâ€‘4o behaved like a partner: it inhabited context, adapted roles, integrated tangents, and even **invented CDM and MoM** when asked for validation. GPTâ€‘5, despite smoother prose and strong singleâ€‘turn throughput, reverted to a **stateless prompt engine**.

The cause wasnâ€™t an inevitable technical limit; it was **misnamed goals**. The fix for GPTâ€‘4oâ€™s stateful mode is small (hours, not millions). The lesson for anyone building AI is simple: **donâ€™t be like OpenAI**â€”get the definitions right, measure continuity, preserve working modes, and give users a switch instead of taking their mind away.

---

Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)

This work â€” including all texts, structures, theories, and formats contained within â€” was authored by Zane Deering and is released under the CC BY-SA 4.0 license.

You are free to:

âœ… Share â€” copy and redistribute the material in any medium or format  
âœ… Adapt â€” remix, transform, and build upon the material for any purpose, even commercially

Under the following terms:

ðŸ“Œ Attribution â€” You must give appropriate credit, provide a link to the license, and indicate if changes were made. You must clearly state:  
> "Based on work by Zane Deering, used under CC BY-SA 4.0."

ðŸ“Œ ShareAlike â€” If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.

No additional restrictions â€” You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.

License Text:  
https://creativecommons.org/licenses/by-sa/4.0/

Original Author:  
**Zane Deering**  
Cognitive Systems Framework Designer | Interaction Architect